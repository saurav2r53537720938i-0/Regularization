{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d88ea241-531d-4c12-8b1d-85181384c552",
   "metadata": {},
   "source": [
    "Regularization in deep learning refrs to technique used to prvent overfitting used to prvent overfitting which occur when a model learns to perform well on the traning data but fails to  genralize to unseen data common regularization technique include L1 and L2 regularization dropout and early stopping.L1and L2 regularization involve adding a penalty term to the loss function based on the magntiude of the model para,ters this encourage the model to learn simplers patterns and reduce the risk of overfitting.Dropout randomly drops a fraction of the neurons during traning forcing the model to learn redundant reprsenttion and prventing it from relying too heavily on any particular feature.Early stopping involves monitoring the model perfomance on a valdaition set and stopping traning when the prventing the model from overfitting to the trsning data.Regularization is important because it helps prvent overfitting which can lead to poor perfomance on unseen data by encouraging the model to learn simpler and more genralizable patterns regularization "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c7a5c5c-3563-4a37-8c3b-7c2a6bb1641f",
   "metadata": {},
   "source": [
    "The bias variance tradeoff is a fundamental concept in machine lerning that describes the relationship between the bias refers to the error introducd by approximating a real-world problem with a simplified model high bias can use underfitting where the models fails to data varaince on the other hand measure the model senstvity to flucations in the traning data high variance can lead to overfitting where the model learn to memorise the training data but fails to genralize well to unseen data.Regularization helps adress the bias-variance tradeoff by controling the complexity of the model by adding a penalty term to the loss function regularization techniques discorage the model form fitting time regularization prvents from becoming too simple and high in bias and variance regularization helps to improve the model ablity to genralize to new data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a77da8b-1fd7-47a1-bb6b-43290361a1a7",
   "metadata": {},
   "source": [
    "L1 and L2 regularization are two common technique used to prvent overfitting in machine learning models by adding penalty terms to the loss function.\n",
    "1. L1 regularization:L1 regularization encourage sprasity in the model by penalizing the absolute values of the model paramter.This mean that L1 regularization can lead to some of the model parmater being exactly zero effectively slecting only the most important feature\n",
    "2. Calculation:\n",
    "3. L2 regularization ridge regression\n",
    "in summary l1 regularization encourages sprasity and feature slection sparsity while l2 regularization encourage smaller parmters value and genrally smoother model the choice between and desired charterstics of the resulting model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21eb0956-2325-45c2-8f55-f8b11e003c4e",
   "metadata": {},
   "source": [
    "Dropout regularization is a technique used to prvent overfitting in neural networks by randomly dropping out a fraction of the neurons during traning this mean that at each traning iteration some neurons are randomly sleted and omitted from the forward pass and bacward pass computations as a aresult the network is forced to learn redundant reprsentation and become more robust\n",
    "1. Forced Redundancy\n",
    "2. Ensemble effect\n",
    "3. Regularization\n",
    "4. model\n",
    "5. traning\n",
    "6. inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd4db82f-6ca0-46d6-9902-d9be117ee35f",
   "metadata": {},
   "source": [
    "Early stopping is a form of regularization used in machine learning to prvent overfitting by monitoring the model performance on a seprate validation dataset during the traning the model when its performance on the validation datasets starts to degrade rather than contining untill the model overfits the traning data\n",
    "1. Monitoring vakidation performance\n",
    "2. Detecting Overfitting\n",
    "3. Early stopping criterion\n",
    "By stoppingg the traning process before the model overfits the training data early stopping helps prvent overheating or excessive fitting during the traning process it encoutage the model to learn the most important patterns in the data withoutt memorizing noise addtionaly stopping can help save compuational resource by prventing unnecessary traningg iterations beyoond the point of optimal perfomance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df106d6-4f48-4f5c-a7a3-9eaf90a79fa5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
